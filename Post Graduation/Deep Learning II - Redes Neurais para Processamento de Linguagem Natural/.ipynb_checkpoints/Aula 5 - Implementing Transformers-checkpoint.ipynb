{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula 5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NCk0lmYdOH1e"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import math\n","import random\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BT58ZXPoXq9A"},"source":["Vamos primeiro editar o nosso Tokenizer para adicionar um token de início e um token de fim."]},{"cell_type":"code","metadata":{"id":"cs89fAPkQNms"},"source":["class Tokenizer:\n","\n","  def __init__(self):\n","    self.dicionario = {}\n","    self.dicionario_inverso = {}\n","    \n","    self.token_desconhecido = 1\n","    self.token_padding = 0\n","    self.max_len = 0\n","\n","  def fit(self, dataset):\n","    # Armazena todas as palavras únicas em um dataset de texto\n","    indice = 4\n","    for texto in dataset:\n","      palavras = texto.split(' ')\n","      self.max_len = len(palavras) + 2 if len(palavras) + 2 > self.max_len else self.max_len\n","      for palavra in palavras:\n","        if palavra not in self.dicionario:\n","          self.dicionario[palavra] = indice\n","          indice += 1\n","\n","    self.dicionario_inverso = { \n","      valor: chave for chave, valor in self.dicionario.items() \n","    }\n","\n","  def encode(self, texto):\n","    # Converte texto para tokens (inteiros)\n","    tokens = texto.split(' ')\n","    texto_tokenizado = []\n","    for token in tokens:\n","      if token not in self.dicionario:\n","        texto_tokenizado.append(self.token_desconhecido)\n","      else:\n","        texto_tokenizado.append(self.dicionario[token])\n","\n","    for _ in range(self.max_len - len(texto_tokenizado)):\n","      texto_tokenizado.append(self.token_padding)\n","  \n","    return texto_tokenizado\n","\n","  def decode(self, tokens):\n","    # Converte tokens para texto\n","    texto = []\n","    for token in tokens:\n","      if token not in self.dicionario_inverso:\n","        texto.append(self.token_desconhecido)\n","      else:\n","        texto.append(self.dicionario_inverso[token])\n","    return ' '.join(texto)\n","\n","  def save(self):\n","    # armazena o dicionario usando json\n","    with open('dicionario.json', 'w') as f:\n","      json.dump(self.dicionario, f)\n","    with open('dicionario_inverso.json', 'w') as f:\n","      json.dump(self.dicionario_inverso, f)\n","\n","  def load(self):\n","    # carrega o dicionario usando json\n","    with open('dicionario.json', 'r') as f:\n","      self.dicionario = json.load(f)\n","    with open('dicionario_inverso.json', 'r') as f:\n","      self.dicionario_inverso = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8Iv5yDmQQc1"},"source":["dataset = [\n","  \"bom celular\",\n","  \"sol, choveu durante o dia\",\n","  \"bom dia, não gostei disso\"\n","]\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit(dataset)\n","tokenized_dataset = [tokenizer.encode(x) for x in dataset]\n","tokenized_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bi9TKbVkZF00"},"source":["Agora podemos codificar nosso Encoding de posição\n","\n","$PE_{(pos, 2i)} = sin(\\frac{pos}{10000^{2i/d}})$ \\\\\n","$PE_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{2i/d}})$"]},{"cell_type":"code","metadata":{"id":"JNmMrbhxOK1e"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dim_model, dropout_p, max_len):\n","      super().__init__()\n","      # Inicializa aqui as camadas e as variáveis necessárias\n","        \n","    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n","      # Realiza o forward do Encoding\n","      raise NotImplementedError('Coloque o código aqui!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVJyQie_Xrgv"},"source":["# Implementando a Tranformer\n","\n","Para esse exercício iremos usar a implementação da transformers do PyTorch para nos adaptarmos em ler a documentação e usar o que já está pronto.\n","\n","A documentação está: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n","\n","\\\\\n","Além dela iremos utilizar o módulo nn.Embedding(), criado pelo PyTorch\n","\n","A documentação está: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"]},{"cell_type":"code","metadata":{"id":"0h6J6aIXOO-W"},"source":["class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        num_tokens,\n","        dim_model,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        dropout_p,\n","    ):\n","      super().__init__()\n","      # Inicializa aqui as camadas e as variáveis necessárias\n","        \n","    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n","      # Realiza o forward do modelo\n","      raise NotImplementedError('Coloque o código aqui!')\n","      \n","    def get_tgt_mask(self, size) -> torch.tensor:\n","      # Cria as máscaras necessárias para não olhar para frente\n","      raise NotImplementedError('Coloque o código aqui!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEixYTtq7yOC"},"source":["Vamos criar o nosso modelo, seu otimizador e a função de custo"]},{"cell_type":"code","metadata":{"id":"pvlh5eIcOwV5"},"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = Transformer(\n","    num_tokens=len(tokenizer.dicionario.keys()) + 4,\n","    dim_model=32, \n","    num_heads=2, \n","    num_encoder_layers=3, \n","    num_decoder_layers=3, \n","    dropout_p=0.1\n",").to(device)\n","opt = torch.optim.SGD(model.parameters(), lr=0.01)\n","loss_fn = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvgJDKai7waa"},"source":["Aqui conseguimos treinar o modelo"]},{"cell_type":"code","metadata":{"id":"c5BtMYoOOyQR"},"source":["from tqdm import tqdm\n","import numpy as np\n","\n","model.train()\n","# Faça o Loop de treino do modelo aqui"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTJz7hKP7uD9"},"source":["E podemos validar o modelo aqui"]},{"cell_type":"code","metadata":{"id":"LQmdPH12O8cI"},"source":["model.eval()\n","\n","x = torch.tensor([[2, 6]], device=device)\n","y_input = torch.tensor([[2]], dtype=torch.long, device=device)\n","\n","for _ in range(7):\n","    # Get source mask\n","    tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n","    \n","    pred = model(x, y_input, tgt_mask)\n","    \n","    next_item = torch.argmax(pred, 2)[-1].item()\n","    next_item = torch.tensor([[next_item]], device=device)\n","\n","    y_input = torch.cat((y_input, next_item), dim=1)\n","\n","    if next_item.view(-1).item() == 3:\n","        break\n","\n","tokenizer.decode(y_input[0, 1:-1].cpu().numpy())"],"execution_count":null,"outputs":[]}]}