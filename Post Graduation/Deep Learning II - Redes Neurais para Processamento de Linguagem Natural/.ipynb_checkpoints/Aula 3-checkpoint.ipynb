{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMbV4_TFT9b4"
   },
   "source": [
    "# Atividade prática - Tokenizador\n",
    "### Proposta por: Nathan S. Gavenski - UOL edtech e PUCRS Online\n",
    "---\n",
    "\n",
    "Nesta atividade iremos criar um Tokenizador simples usando apenas Python e nenhuma outra biblioteca. \n",
    "Todo o código extra para a validação da nossa implementação já está posto nas células.\n",
    "Para esta atividade basta apenas implementar onde os comentários `#TODO` estão ecritos.\n",
    "\n",
    "Vamos usar o seguinte dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vn2E09CyRY3i"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "  \"bom celular\",\n",
    "  \"sol, choveu durante o dia\",\n",
    "  \"bom dia, não gostei disso\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEAZ_gR2V9aP"
   },
   "source": [
    "Agora vamos implementar o tokenizador.\n",
    "A classe possui 5 funções diferentes:\n",
    "\n",
    "\n",
    "1.   `fit` - Armazena todas as palavras únicas em um dataset de texto\n",
    "2.   `encode` - codifica texto para tokens\n",
    "3.   `decode` - decodifica tokens para texto\n",
    "4.   `save` - armazena o dicionário em um arquivo JSON\n",
    "5.   `load` - carrega o dicionário em memória\n",
    "\n",
    "Preencha cada função com o código necessário para a classe funcionar como planejado.\n",
    "\n",
    "OBS: A função `__init__` deve ser alterada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "MD_hRdjYX7_x"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.dicionario = {}\n",
    "    self.dicionario_inverso = {}\n",
    "    self.unknown_token = 1\n",
    "\n",
    "  def fit(self, dataset):\n",
    "    for texto in dataset:\n",
    "        texto = texto.replace(',', '')\n",
    "        texto = texto.replace('.', '')\n",
    "        palavras = texto.split(' ')\n",
    "        for palavra in palavras:\n",
    "            if palavra not in self.dicionario.keys():\n",
    "                self.dicionario[palavra] = len(self.dicionario.keys())+2\n",
    "\n",
    "    self.dicionario_inverso = {value: key for key, value in self.dicionario.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    assert len(self.dicionario) > 0, \"Tokenizer must be fitted first\"\n",
    "    tokens = text.split(' ')\n",
    "    tokenized_text = []\n",
    "    for token in tokens:\n",
    "        if token not in dicionario.keys():\n",
    "            tokenized_text.append(self.unknown_token)\n",
    "        else:\n",
    "            tokenized_text.append(self.dicionario[token])\n",
    "    return tokenized_text\n",
    "\n",
    "  def decode(self, tokens):\n",
    "    assert len(self.dicionario) > 0, \"Tokenizer must be fitted first\"\n",
    "    decoded_text = []\n",
    "    for token in tokens:\n",
    "        if token not in self.dicionario_inverso.keys():\n",
    "            decoded_text.append(\"???\")\n",
    "        else:\n",
    "            decoded_text.append(self.dicionario_inverso[token])\n",
    "    return \" \".join(decoded_text)\n",
    "\n",
    "  def save(self):\n",
    "    with open('dicionario.json', 'w') as file:\n",
    "        json.dump(self.dicionario, file)\n",
    "    with open('dicionario_inverso.json', 'w') as file:\n",
    "        json.dump(self.dicionario_inverso, file)\n",
    "\n",
    "  def load(self, dicionario, dicionario_inverso):\n",
    "    with open(dicionario, 'r') as file:\n",
    "        self.dicionario = json.load(file)\n",
    "    with open(dicionario_inverso, 'r') as file:\n",
    "        self.dicionario_inverso = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI33ECMfZO_W"
   },
   "source": [
    "Vamos instânciar aqui o tokenizador e fazer ele aprender nosso dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bom': 2, 'celular': 3, 'sol': 4, 'choveu': 5, 'durante': 6, 'o': 7, 'dia': 8, 'não': 9, 'gostei': 10, 'disso': 11}\n",
      "{2: 'bom', 3: 'celular', 4: 'sol', 5: 'choveu', 6: 'durante', 7: 'o', 8: 'dia', 9: 'não', 10: 'gostei', 11: 'disso'}\n",
      "[6, 4, 5, 3, 1, 1, 1]\n",
      "??? choveu o ??? bom ??? ??? ???\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "tokenizador = Tokenizer()\n",
    "tokenizador.fit(dataset)\n",
    "print(tokenizador.dicionario)\n",
    "print(tokenizador.dicionario_inverso)\n",
    "print(tokenizador.encode(\"durante sol choveu celular cachorro romeu julia\"))\n",
    "print(tokenizador.decode([0, 5, 7, 1, 2, 1, 1000, 673]))\n",
    "tokenizador.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyWImAWojGch"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(dataset)\n",
    "tokenizer.dicionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI830OL2ZW1_"
   },
   "source": [
    "Nas próximas células vamos testar a nossa implementação.\n",
    "A palavra `assert` serve para validarmos uma expressão lógica.\n",
    "Na primeira célula, estamos validando se os dicionários são iguais, enquanto na segunda validamos se o resultado da codificação é igual ao que esperamos. \n",
    "Finalmente, na última célula, vamos validar se ao codificarmos e decodificarmos um texto, o resultado é igual a sentença original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eP-rIhKTHJD"
   },
   "outputs": [],
   "source": [
    "dicionario = {\n",
    "    \"bom\": 0,\n",
    "    \"celular\": 1, \n",
    "    \"sol,\": 2,\n",
    "    \"choveu\": 3,\n",
    "    \"durante\": 4,\n",
    "    \"o\": 5,\n",
    "    \"dia\": 6,\n",
    "    \"dia,\": 7,\n",
    "    \"não\": 8,\n",
    "    \"gostei\": 9,\n",
    "    \"disso\": 10,\n",
    "}\n",
    "\n",
    "assert tokenizer.dicionario == dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3W0WtV9T2Qu"
   },
   "outputs": [],
   "source": [
    "expected_output = [\n",
    "  [0, 1],\n",
    "  [2, 3, 4, 5, 6],\n",
    "  [0, 7, 8, 9, 10],\n",
    "]\n",
    "\n",
    "for i in range(len(expected_output)):\n",
    "  assert tokenizer.encode(dataset[i]) == expected_output[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oEUT3nLS2kT"
   },
   "outputs": [],
   "source": [
    "assert tokenizer.decode(tokenizer.encode(dataset[0])) == dataset[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5AvA6iyT8AV"
   },
   "source": [
    "# E se tentarmos codificar uma palavra que nunca vimos? \n",
    "# Ou se tivermos tamanhos diferentes?\n",
    "\n",
    "Agora precisamos mudar o código que criamos para conseguir lidar com palavras desconhecidas e padding.\n",
    "Depois de realizar as alterações use as próximas células para validar a sua implementação.\n",
    "\n",
    "OBS: Padding = 0 e Token Desconhecido = 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
