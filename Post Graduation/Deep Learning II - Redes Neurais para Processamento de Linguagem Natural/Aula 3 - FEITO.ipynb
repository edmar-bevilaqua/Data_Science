{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula 3 - FEITO.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tMbV4_TFT9b4"},"source":["# Atividade prática - Tokenizador\n","###### Por Nathan S. Gavenski - UOL edtech e PUCRS Online\n","\\\\\n","\n","Nesta atividade iremos criar um Tokenizador simples usando apenas Python e nenhuma outra biblioteca. \n","Todo o código extra para a validação da nossa implementação já está posto nas células.\n","Para esta atividade basta apenas implementar onde os comentários `#TODO` estão ecritos.\n","\n","Vamos usar o seguinte dataset:\n","\n"]},{"cell_type":"code","metadata":{"id":"Vn2E09CyRY3i"},"source":["dataset = [\n","  \"bom celular\",\n","  \"sol, choveu durante o dia\",\n","  \"bom dia, não gostei disso\",\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEAZ_gR2V9aP"},"source":["Agora vamos implementar o tokenizador.\n","A classe possui 5 funções diferentes:\n","\n","\n","1.   `fit` - Armazena todas as palavras únicas em um dataset de texto\n","2.   `encode` - codifica texto para tokens\n","3.   `decode` - decodifica tokens para texto\n","4.   `save` - armazena o dicionário em um arquivo JSON\n","5.   `load` - carrega o dicionário em memória\n","\n","Preencha cada função com o código necessário para a classe funcionar como planejado.\n","\n","OBS: A função `__init__` deve ser alterada\n"]},{"cell_type":"code","metadata":{"id":"MD_hRdjYX7_x"},"source":["class Tokenizer:\n","\n","  def __init__(self):\n","    self.dicionario = {}\n","    self.dicionario_inverso = {}\n","    self.token_desconhecido = 1\n","\n","  def fit(self, dataset):\n","    # Armazena todas as palavras únicas em um dataset de texto\n","    indice = 0\n","    for texto in dataset:\n","      palavras = texto.split(' ')\n","      for palavra in palavras:\n","        if palavra not in self.dicionario:\n","          self.dicionario[palavra] = indice\n","          indice += 1\n","\n","    self.dicionario_inverso = { \n","      valor: chave for chave, valor in self.dicionario.items() \n","    }\n","\n","  def encode(self, texto):\n","    # Converte texto para tokens (inteiros)\n","    tokens = texto.split(' ')\n","    texto_tokenizado = []\n","    for token in tokens:\n","      if token not in self.dicionario:\n","        texto_tokenizado.append(self.token_desconhecido)\n","      else:\n","        texto_tokenizado.append(self.dicionario[token])\n","    return texto_tokenizado\n","\n","  def decode(self, tokens):\n","    # Converte tokens para texto\n","    texto = []\n","    for token in tokens:\n","      if token not in self.dicionario_inverso:\n","        texto.append(self.token_desconhecido)\n","      else:\n","        texto.append(self.dicionario_inverso[token])\n","    return ' '.join(texto)\n","\n","  def save(self):\n","    # armazena o dicionario usando json\n","    with open('dicionario.json', 'w') as f:\n","      json.dump(self.dicionario, f)\n","    with open('dicionario_inverso.json', 'w') as f:\n","      json.dump(self.dicionario_inverso, f)\n","\n","  def load(self):\n","    # carrega o dicionario usando json\n","    with open('dicionario.json', 'r') as f:\n","      self.dicionario = json.load(f)\n","    with open('dicionario_inverso.json', 'r') as f:\n","      self.dicionario_inverso = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pI33ECMfZO_W"},"source":["Vamos instânciar aqui o tokenizador e fazer ele aprender nosso dataset"]},{"cell_type":"code","metadata":{"id":"jyWImAWojGch","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633636267485,"user_tz":180,"elapsed":253,"user":{"displayName":"Nathan Schneider Gavenski","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivQ8XlubPN7xwmI_Gxut2AErfwpSrKuVgDYZVCsQ=s64","userId":"17775166441917814302"}},"outputId":"0875abe1-66a0-49f5-d595-3047c3fe625c"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit(dataset)\n","tokenizer.dicionario"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bom': 0,\n"," 'celular': 1,\n"," 'choveu': 3,\n"," 'dia': 6,\n"," 'dia,': 7,\n"," 'disso': 10,\n"," 'durante': 4,\n"," 'gostei': 9,\n"," 'não': 8,\n"," 'o': 5,\n"," 'sol,': 2}"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"qI830OL2ZW1_"},"source":["Nas próximas células vamos testar a nossa implementação.\n","A palavra `assert` serve para validarmos uma expressão lógica.\n","Na primeira célula, estamos validando se os dicionários são iguais, enquanto na segunda validamos se o resultado da codificação é igual ao que esperamos. \n","Finalmente, na última célula, vamos validar se ao codificarmos e decodificarmos um texto, o resultado é igual a sentença original. "]},{"cell_type":"code","metadata":{"id":"6eP-rIhKTHJD"},"source":["dicionario = {\n","    \"bom\": 0,\n","    \"celular\": 1, \n","    \"sol,\": 2,\n","    \"choveu\": 3,\n","    \"durante\": 4,\n","    \"o\": 5,\n","    \"dia\": 6,\n","    \"dia,\": 7,\n","    \"não\": 8,\n","    \"gostei\": 9,\n","    \"disso\": 10,\n","}\n","\n","assert tokenizer.dicionario == dicionario"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3W0WtV9T2Qu"},"source":["expected_output = [\n","  [0, 1],\n","  [2, 3, 4, 5, 6],\n","  [0, 7, 8, 9, 10],\n","]\n","\n","for i in range(len(expected_output)):\n","  assert tokenizer.encode(dataset[i]) == expected_output[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oEUT3nLS2kT"},"source":["assert tokenizer.decode(tokenizer.encode(dataset[0])) == dataset[0] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J5AvA6iyT8AV"},"source":["# E se tentarmos codificar uma palavra que nunca vimos? \n","# Ou se tivermos tamanhos diferentes?\n","\n","Agora precisamos mudar o código que criamos para conseguir lidar com palavras desconhecidas e padding.\n","Depois de realizar as alterações use as próximas células para validar a sua implementação.\n","\n","OBS: Padding = 0 e Token Desconhecido = 1"]},{"cell_type":"code","metadata":{"id":"O97OtXokd9Ey"},"source":["dicionario = {\n","    \"bom\": 2,\n","    \"celular\": 3, \n","    \"sol,\": 4,\n","    \"choveu\": 5,\n","    \"durante\": 6,\n","    \"o\": 7,\n","    \"dia\": 8,\n","    \"dia,\": 9,\n","    \"não\": 10,\n","    \"gostei\": 11,\n","    \"disso\": 12,\n","}\n","\n","assert tokenizer.dicionario == dicionario"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ut7TPcAyf-9X"},"source":["expected_output = [\n","  [2, 3],\n","  [4, 5, 6, 7, 8],\n","  [2, 9, 10, 11, 12],\n","]\n","\n","for i in range(len(expected_output)):\n","  assert tokenizer.encode(dataset[i]) == expected_output[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogA3uAIHleiJ"},"source":["assert tokenizer.encode(\"gostei muito disso\") == [11, 1, 12]\n","assert tokenizer.decode(tokenizer.encode(dataset[0])) == dataset[0]"],"execution_count":null,"outputs":[]}]}